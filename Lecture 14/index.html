<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content=
      "width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Computational Physics Lecture 14</title>
    <link rel="stylesheet" href="../dist/reset.css">
    <link rel="stylesheet" href="../dist/reveal.css">
    <link rel="stylesheet" href="../dist/theme/serif.css"
      id="theme">
    <!-- <link rel="stylesheet" href="./dist/theme/night.css" id="theme"> -->
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href=
      "../plugin/highlight/monokai.css" id="highlight-theme">
    <link rel="stylesheet" href=
      "../css/style.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2>Numerical Linear Algebra</h2>
        </section>

        <section>
          <h2>Numerical Linear Algebra</h2>
          <p>Linear algebra is an important part of a computational physicist's
          tool box. This includes:</p>
          <ul>
            <li>Solving systems of linear (matrix) equations;</li>
            <li>Computing the inverse of a matrix, and/or its determinant;</li>
            <li>Linear least-squares problem;</li>
            <li>Diagonalizing matrices, finding eigenvalues and eigenvectors;</li>
          </ul>
        </section>

        <section>
          <h2>Numerical Linear Algebra</h2>
          <p>Many numerical linear algebra algorithms have been implemented
          in BLAS and LAPACK:</p>
          <ul>
            <li>BLAS (Basic Linear Algebra Subprograms) provides basic
            matrix-vector or matrix-matrix operations.</li>
            <li>LAPACK (Linear Algebra PACKage) provides solvers of linear
            systems, eigenvalue problems, and matrix factorization algorithms.</li>
          </ul>
          <p>These libraries are written in Fortran, and are available on most
          platforms. Most languages provide wrappers for these libraries, so it
          doesn't make sense writing one's own.</p>
        </section>

        <section>
          <h2>Numerical Linear Algebra</h2>
          <p>For C++, there is a great modern library called <em>Eigen</em>.</p>
          <a href="https://eigen.tuxfamily.org/index.php">Eigen Homepage</a>
          <p>The Matrix and Vector classes are native C++ constructs, and far
          easier to use than a Fortran library.</p>
        </section>

        <section>
          <h2>Numerical Linear Algebra</h2>
          <p>For Python, <code>numpy.linalg</code> and <code>scipy.linalg</code> provide
          linear algebra functionality similar to BLAS and LAPACK.</p>
          <p class="fragment">Our goal for the next few lectures is to provide
          you with a map for navigating these libraries, explaining what each
          algorithm does, so that you have an idea what to use when you need them.</p>
        </section>

        <section>
          <h2>Solving Linear Systems</h2>
          <p>The most basic task in linear algebra is solving a linear system. A
          linear system of equations is a set of equations of the form</p>
          $$
          \begin{align*}
            a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1 \\
            a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2 \\
            \vdots &= \vdots \\
            a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= b_n
          \end{align*}
          $$
          <p>where $a_{ij}$ and $b_i$ are known numbers, and $x_i$ are unknowns.</p>
        </section>

        <section>
          <h2>Solving Linear Systems</h2>
          <p>Linear systems can be written in matrix form:</p>
          $$
          \mathbf{A}\mathbf{x} = \mathbf{b}
          $$
          <p>where $\mathbf{A}$ is an $m\times n$ matrix, and both $\mathbf{x}$
          and $\mathbf{b}$ are $n$-dimensional vectors.</p>
          <div class="fragment">
            <p>If $m > n$, the system is over-determined and there may not be a
            solution. If $m < n$, the system is under-determined and will in
            general have infinite solutions.</p>
          </div>
        </section>

        <section>
          <h2>Solving Linear Systems</h2>
          <p>Even when square matrices $m = n$, the system may be
          under-determined if the matrix is singular. This happens if one or
          more of the equations can be written as a linear combination of other
          equations.</p>
          <p class="fragment">When the system is under-determined, finding the <em>solution space</em> involves
          finding the Singular Value Decomposition (SVD) of the matrix</p>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>One of the most basic algorithms for solving linear systems is
          Gaussian Elimination with backsubstitution.</p>
          <div class="fragment">
            <p>Consider the following matrix equation:</p>
            $$
            \begin{pmatrix}
              1 & 2 & 3 \\
              4 & 6 & 7 \\
              5 & 8 & 9
            \end{pmatrix}
            \begin{pmatrix}
              x_1 \\
              x_2 \\
              x_3
            \end{pmatrix}
            =
            \begin{pmatrix}
              1 \\
              2 \\
              4
            \end{pmatrix}
            $$
            <p class="fragment">
              We can eliminate the first column by subtracting the first row
              from the second and third rows
            </p>
          </div>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>After the first step, the matrix equation becomes:</p>
          $$
          \begin{pmatrix}
            1 & 2 & 3 \\
            0 & -2 & -5 \\
            0 & -2 & -6
          \end{pmatrix}
          \begin{pmatrix}
            x_1 \\
            x_2 \\
            x_3
          \end{pmatrix}
          =
          \begin{pmatrix}
            1 \\
            -2 \\
            -1
          \end{pmatrix}
          $$
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>Next, we eliminate the second column by subtracting the second row
          from the third row:</p>
          $$
          \begin{pmatrix}
            1 & 2 & 3 \\
            0 & -2 & -5 \\
            0 & 0 & -1
          \end{pmatrix}
          \begin{pmatrix}
            x_1 \\
            x_2 \\
            x_3
          \end{pmatrix}
          =
          \begin{pmatrix}
            1 \\
            -2 \\
            1
          \end{pmatrix}
          $$
          <p class="fragment">Now our matrix is in an upper-triangular form. We
          are ready for backsubstitution.</p>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>First we can solve the last equation for $x_3$:</p>
          $$
          -x_3 = 1 \implies x_3 = -1
          $$
          <div class="fragment">
            <p>Then we can substitute this into the second equation to solve for $x_2$:</p>
            $$
            -2x_2 - 5(-1) = -2 \implies x_2 = 3.5
            $$
            <div class="fragment">
              <p>Finally, we substitute the above into the first equation to solve for $x_1$:</p>
              $$
              x_1 + 2(3.5) + 3(-1) = 1 \implies x_1 = -3
              $$
            </div>
          </div>
        </section>

        <section>
          <h2>Pivoting</h2>
          <p>What happens when the matrix looks like this:</p>
          $$
          \begin{pmatrix}
            0 & 2 & 3 \\
            4 & 6 & 7 \\
            5 & 8 & 9
          \end{pmatrix}
          \begin{pmatrix}
            x_1 \\
            x_2 \\
            x_3
          \end{pmatrix}
          =
          \begin{pmatrix}
            1 \\
            2 \\
            4
          \end{pmatrix}
          $$
          <p class="fragment">Subtracting the first row from the second does not
          eliminate the first element of the second row.</p>
        </section>

        <section>
          <h2>Pivoting</h2>
          <p>We can fix this by swapping the first and the third row (the row
          with the largest leading coefficient):</p>
          $$
          \begin{pmatrix}
            5 & 8 & 9 \\
            4 & 6 & 7 \\
            0 & 2 & 3
          \end{pmatrix}
          \begin{pmatrix}
            x_1 \\
            x_2 \\
            x_3
          \end{pmatrix}
          =
          \begin{pmatrix}
            4 \\
            2 \\
            1
          \end{pmatrix}
          $$
          <p>Note that you will need to swap the corresponding elements of the
          right hand side too! This procedure is equivalent to rearranging the order of the equations,
          which does not affect the solution.</p>
        </section>

        <section>
          <h2>Pivoting</h2>
          <p>Exchanging rows only is called <em>partial pivoting</em>, while
          exchanging both rows and columns is called <em>full pivoting</em>.</p>
          <p class="fragment">When exchanging rows, we need to swap the right
          hand side; when exchanging columns, we need to exchange the order of
          the unknown variables.</p>
          <p class="fragment">A way to do partial pivoting systematically is to always move
          the row with the <em>largest</em> diagonal element up to the current row.</p>
        </section>

        <!-- <section>
             <h2>LU Decomposition</h2>
             </section>
        -->
        <section>
          <h2>Gaussian Elimination</h2>
          <p>The goal of Gaussian elimination is to give you an upper
          triangular matrix:</p>
          <p>$$
            \begin{pmatrix}
              u_{11} & u_{12} & u_{13} & \dots & u_{1n} \\
              0 & u_{22} & u_{23} & \dots & u_{2n} \\
              0 & 0 & u_{33} & \dots & u_{3n} \\
              \vdots & \vdots & \vdots & \ddots & \vdots \\
              0 & 0 & 0 & \dots & u_{nn}
            \end{pmatrix}
          $$</p>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>We can formalize this process using matrix multiplications. The
          first step of Gaussian elimination is to get rid of the first column (except the diagonal element):</p>
          <div style="font-size: 80%">
          $$
            \frac{1}{a_{11}}
            \begin{pmatrix}
              a_{11} & 0 & 0 & \dots & 0 \\
              -a_{21} & a_{11} & 0 & \dots & 0 \\
              -a_{31} & 0 & a_{11} & \dots & 0 \\
              \vdots & \vdots & \vdots & \ddots & \vdots \\
              -a_{n1} & 0 & 0 & \dots & a_{11}
            \end{pmatrix}
            \begin{pmatrix}
              a_{11} & a_{12} & a_{13} & \dots & a_{1n} \\
              a_{21} & a_{22} & a_{23} & \dots & a_{2n} \\
              a_{31} & a_{32} & a_{33} & \dots & a_{3n} \\
              \vdots & \vdots & \vdots & \ddots & \vdots \\
              a_{n1} & a_{n2} & a_{n3} & \dots & a_{nn}
            \end{pmatrix}
            =
            \begin{pmatrix}
              a_{11} & a_{12} & a_{13} & \dots & a_{1n} \\
              0 & a'_{22} & a'_{23} & \dots & a'_{2n} \\
              0 & a'_{32} & a'_{33} & \dots & a'_{3n} \\
              \vdots & \vdots & \vdots & \ddots & \vdots \\
              0 & a'_{n2} & a'_{n3} & \dots & a'_{nn}
            \end{pmatrix}
          $$
          </div>
          <p>We denote the lower-triangular matrix $\mathbf{L}_1$.</p>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>The next step of Gaussian elimination can be written in a similar way:</p>
          <div style="font-size: 80%">
            $$
              \frac{1}{a'_{22}}
              \begin{pmatrix}
                a'_{22} & 0 & 0 & \dots & 0 \\
                0 & a'_{22} & 0 & \dots & 0 \\
                0 & -a'_{32} & a'_{22} & \dots & 0 \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                0 & -a'_{n2} & 0 & \dots & a'_{22}
              \end{pmatrix}
              \begin{pmatrix}
                a'_{11} & a'_{12} & a'_{13} & \dots & a'_{1n} \\
                0 & a'_{22} & a'_{23} & \dots & a'_{2n} \\
                0 & a'_{32} & a'_{33} & \dots & a'_{3n} \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                0 & a'_{n2} & a'_{n3} & \dots & a'_{nn}
              \end{pmatrix}
              =
              \begin{pmatrix}
                a'_{11} & a'_{12} & a'_{13} & \dots & a'_{1n} \\
                0 & a'_{22} & a'_{23} & \dots & a'_{2n} \\
                0 & 0 & a''_{33} & \dots & a''_{3n} \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & a''_{n3} & \dots & a''_{nn}
              \end{pmatrix}
            $$
          </div>
          <p>We denote the lower-triangular matrix $\mathbf{L}_2$.</p>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>After $n-1$ steps, we get:</p>
          $$
            \mathbf{L}_{n-1}\dots\mathbf{L}_2\mathbf{L}_1\mathbf{A} = \mathbf{U}
          $$
          <p>where $\mathbf{U}$ is upper-triangular.</p>
          <div class="fragment">
            <p>Up to now, this is simply a formalization of Gaussian elimination.
            If all the matrices $\mathbf{L}_i$ are invertible, we can write:</p>
            $$
              \mathbf{A} = \left(\mathbf{L}_1^{-1}\mathbf{L}_2^{-1}\dots\mathbf{L}_{n-1}^{-1}\right)\mathbf{U}
            $$
          </div>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>In fact, the inverse of the lower-triangular matrices are lower-triangular too:</p>
          <div style="font-size: 80%">
            $$
              \mathbf{L}_1^{-1} =
              \begin{pmatrix}
                1 & 0 & 0 & \dots & 0 \\
                -a_{21}/a_{11} & 1 & 0 & \dots & 0 \\
                -a_{31}/a_{11} & 0 & 1 & \dots & 0 \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                -a_{n1}/a_{11} & 0 & 0 & \dots & 1
              \end{pmatrix}^{-1}
              =
              \begin{pmatrix}
                1 & 0 & 0 & \dots & 0 \\
                a_{21}/a_{11} & 1 & 0 & \dots & 0 \\
                a_{31}/a_{11} & 0 & 1 & \dots & 0 \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                a_{n1}/a_{11} & 0 & 0 & \dots & 1
              \end{pmatrix}
            $$
          </div>
          <div class="fragment">
          <p>The product of the inverses of the lower triangular matrices
            $\mathbf{L}_i$ is again lower triangular, so we can write:</p>
          $$
            \mathbf{A} = \left(\mathbf{L}_1^{-1}\mathbf{L}_2^{-1}\dots\mathbf{L}_{n-1}^{-1}\right) = \mathbf{L}\mathbf{U}
          $$
          <p>This is called the <em>LU decomposition</em> (or factorization) of $\mathbf{A}$.</p>
          </div>
        </section>

      </div>
    </div>
    <!-- <script src="js/head.min.js"></script> -->
    <!-- <script>
         head.js(
         "vendor/jquery.min.js",
         // "vendor/three.js/build/three.min.js",
         // "vendor/three.js/examples/js/controls/OrbitControls.js",
         /* "vendor/socket.io-client/dist/socket.io.js", */
         // "vendor/THREE.MeshLine.js",
         /* "vendor/dat.gui.min.js",*/
         "vendor/EventEmitter.js",
         // "js/samples.js",
         );
         </script> -->
    <script src="../dist/reveal.js"></script>
    <script src="../plugin/math/math.js"></script>
    <script src="../plugin/notes/notes.js"></script>
    <script src="../plugin/markdown/markdown.js"></script>
    <script src="../plugin/highlight/highlight.js"></script>

    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        transition: "fade",
        slideNumber: true,
        history: true,
        center: true,
        width: "100%",
        height: "100%",
        disableLayout: false,
        navigationMode: "default",
        // minScale: 1,
        // maxScale: 1,
        margin: 0.2,
        padding: 0,
        hash: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          // pass other options into `MathJax.Hub.Config()`
          TeX: { Macros: { RR: "{\\bf R}" } }
        },
        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
      });
      Reveal.configure({
        keyboard: {
          81: null, // Do not respond to Q
          // 32: null, // Do not respond to space
        }
      });
      // Reveal.addEventListener("slidechanged", function(event) {
      //   var foot1 = document.getElementById("footer1");
      //   var foot2 = document.getElementById("footer2");
      //
      //   if (Reveal.getIndices().h === 0) {
      //     foot1.style.display = "none";
      //     foot2.style.display = "none";
      //   } else {
      //     foot1.style.display = "block";
      //     foot2.style.display = "block";
      //   }
      // });
      //
    </script>
  </body>
</html>
