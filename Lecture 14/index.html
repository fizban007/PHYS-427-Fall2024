<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content=
      "width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Computational Physics Lecture 14</title>
    <link rel="stylesheet" href="../dist/reset.css">
    <link rel="stylesheet" href="../dist/reveal.css">
    <link rel="stylesheet" href="../dist/theme/serif.css"
      id="theme">
    <!-- <link rel="stylesheet" href="./dist/theme/night.css" id="theme"> -->
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href=
      "../plugin/highlight/monokai.css" id="highlight-theme">
    <link rel="stylesheet" href=
      "../css/style.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2>Numerical Linear Algebra</h2>
        </section>

        <section>
          <h2>Numerical Linear Algebra</h2>
          <p>Linear algebra is an important part of a computational physicist's
          tool box. This includes:</p>
          <ul>
            <li>Solving systems of linear (matrix) equations;</li>
            <li>Computing the inverse of a matrix, and/or its determinant;</li>
            <li>Linear least-squares problem;</li>
            <li>Diagonalizing matrices, finding eigenvalues and eigenvectors;</li>
          </ul>
        </section>

        <section>
          <h2>Numerical Linear Algebra</h2>
          <p>Many numerical linear algebra algorithms have been implemented
          in BLAS and LAPACK:</p>
          <ul>
            <li>BLAS (Basic Linear Algebra Subprograms) provides basic
            matrix-vector or matrix-matrix operations.</li>
            <li>LAPACK (Linear Algebra PACKage) provides solvers of linear
            systems, eigenvalue problems, and matrix factorization algorithms.</li>
          </ul>
          <p>These libraries are written in Fortran, and are available on most
          platforms. Most languages provide wrappers for these libraries, so it
          doesn't make sense writing one's own.</p>
        </section>

        <section>
          <h2>Numerical Linear Algebra</h2>
          <p>For C++, there is a great modern library called <em>Eigen</em>.</p>
          <a href="https://eigen.tuxfamily.org/index.php">Eigen Homepage</a>
          <p>The Matrix and Vector classes are native C++ constructs, and far
          easier to use than a Fortran library.</p>
        </section>

        <section>
          <h2>Numerical Linear Algebra</h2>
          <p>For Python, <code>numpy.linalg</code> and <code>scipy.linalg</code> provide
          linear algebra functionality similar to BLAS and LAPACK.</p>
          <p class="fragment">Our goal for the next few lectures is to provide
          you with a map for navigating these libraries, explaining what each
          algorithm does, so that you have an idea what to use when you need them.</p>
        </section>

        <section>
          <h2>Solving Linear Systems</h2>
          <p>The most basic task in linear algebra is solving a linear system. A
          linear system of equations is a set of equations of the form</p>
          $$
          \begin{align*}
            a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1 \\
            a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2 \\
            \vdots &= \vdots \\
            a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= b_n
          \end{align*}
          $$
          <p>where $a_{ij}$ and $b_i$ are known numbers, and $x_i$ are unknowns.</p>
        </section>

        <section>
          <h2>Solving Linear Systems</h2>
          <p>Linear systems can be written in matrix form:</p>
          $$
          \mathbf{A}\mathbf{x} = \mathbf{b}
          $$
          <p>where $\mathbf{A}$ is an $m\times n$ matrix, and both $\mathbf{x}$
          and $\mathbf{b}$ are $n$-dimensional vectors.</p>
          <div class="fragment">
            <p>If $m > n$, the system is over-determined and there may not be a
            solution. If $m < n$, the system is under-determined and will in
            general have infinite solutions.</p>
          </div>
        </section>

        <section>
          <h2>Solving Linear Systems</h2>
          <p>Even when square matrices $m = n$, the system may be
          under-determined if the matrix is singular. This happens if one or
          more of the equations can be written as a linear combination of other
          equations.</p>
          <p class="fragment">When the system is under-determined, finding the <em>solution space</em> involves
          finding the Singular Value Decomposition (SVD) of the matrix</p>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>One of the most basic algorithms for solving linear systems is
          Gaussian Elimination with backsubstitution.</p>
          <div class="fragment">
            <p>Consider the following matrix equation:</p>
            $$
            \begin{pmatrix}
              1 & 2 & 3 \\
              4 & 6 & 7 \\
              5 & 8 & 9
            \end{pmatrix}
            \begin{pmatrix}
              x_1 \\
              x_2 \\
              x_3
            \end{pmatrix}
            =
            \begin{pmatrix}
              1 \\
              2 \\
              4
            \end{pmatrix}
            $$
            <p class="fragment">
              We can eliminate the first column by subtracting the first row
              from the second and third rows
            </p>
          </div>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>After the first step, the matrix equation becomes:</p>
          $$
          \begin{pmatrix}
            1 & 2 & 3 \\
            0 & -2 & -5 \\
            0 & -2 & -6
          \end{pmatrix}
          \begin{pmatrix}
            x_1 \\
            x_2 \\
            x_3
          \end{pmatrix}
          =
          \begin{pmatrix}
            1 \\
            -2 \\
            -1
          \end{pmatrix}
          $$
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>Next, we eliminate the second column by subtracting the second row
          from the third row:</p>
          $$
          \begin{pmatrix}
            1 & 2 & 3 \\
            0 & -2 & -5 \\
            0 & 0 & -1
          \end{pmatrix}
          \begin{pmatrix}
            x_1 \\
            x_2 \\
            x_3
          \end{pmatrix}
          =
          \begin{pmatrix}
            1 \\
            -2 \\
            1
          \end{pmatrix}
          $$
          <p class="fragment">Now our matrix is in an upper-triangular form. We
          are ready for backsubstitution.</p>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>First we can solve the last equation for $x_3$:</p>
          $$
          -x_3 = 1 \implies x_3 = -1
          $$
          <div class="fragment">
            <p>Then we can substitute this into the second equation to solve for $x_2$:</p>
            $$
            -2x_2 - 5(-1) = -2 \implies x_2 = 3.5
            $$
            <div class="fragment">
              <p>Finally, we substitute the above into the first equation to solve for $x_1$:</p>
              $$
              x_1 + 2(3.5) + 3(-1) = 1 \implies x_1 = -3
              $$
            </div>
          </div>
        </section>

        <section>
          <h2>Pivoting</h2>
          <p>What happens when the matrix looks like this:</p>
          $$
          \begin{pmatrix}
            0 & 2 & 3 \\
            4 & 6 & 7 \\
            5 & 8 & 9
          \end{pmatrix}
          \begin{pmatrix}
            x_1 \\
            x_2 \\
            x_3
          \end{pmatrix}
          =
          \begin{pmatrix}
            1 \\
            2 \\
            4
          \end{pmatrix}
          $$
          <p class="fragment">Subtracting the first row from the second does not
          eliminate the first element of the second row.</p>
        </section>

        <section>
          <h2>Pivoting</h2>
          <p>We can fix this by swapping the first and the third row (the row
          with the largest leading coefficient):</p>
          $$
          \begin{pmatrix}
            5 & 8 & 9 \\
            4 & 6 & 7 \\
            0 & 2 & 3
          \end{pmatrix}
          \begin{pmatrix}
            x_1 \\
            x_2 \\
            x_3
          \end{pmatrix}
          =
          \begin{pmatrix}
            4 \\
            2 \\
            1
          \end{pmatrix}
          $$
          <p>Note that you will need to swap the corresponding elements of the
          right hand side too! This procedure is equivalent to rearranging the order of the equations,
          which does not affect the solution.</p>
        </section>

        <section>
          <h2>Pivoting</h2>
          <p>Exchanging rows only is called <em>partial pivoting</em>, while
          exchanging both rows and columns is called <em>full pivoting</em>.</p>
          <p class="fragment">When exchanging rows, we need to swap the right
          hand side; when exchanging columns, we need to exchange the order of
          the unknown variables.</p>
          <p class="fragment">A way to do partial pivoting systematically is to always move
          the row with the <em>largest</em> diagonal element up to the current row.</p>
        </section>

        <!-- <section>
             <h2>LU Decomposition</h2>
             </section>
        -->
        <section>
          <h2>Gaussian Elimination</h2>
          <p>The goal of Gaussian elimination is to give you an upper
          triangular matrix:</p>
          <p>$$
            \begin{pmatrix}
              u_{11} & u_{12} & u_{13} & \dots & u_{1n} \\
              0 & u_{22} & u_{23} & \dots & u_{2n} \\
              0 & 0 & u_{33} & \dots & u_{3n} \\
              \vdots & \vdots & \vdots & \ddots & \vdots \\
              0 & 0 & 0 & \dots & u_{nn}
            \end{pmatrix}
          $$</p>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>We can formalize this process using matrix multiplications. The
          first step of Gaussian elimination is to get rid of the first column (except the diagonal element):</p>
          <div style="font-size: 80%">
          $$
            \frac{1}{a_{11}}
            \begin{pmatrix}
              a_{11} & 0 & 0 & \dots & 0 \\
              -a_{21} & a_{11} & 0 & \dots & 0 \\
              -a_{31} & 0 & a_{11} & \dots & 0 \\
              \vdots & \vdots & \vdots & \ddots & \vdots \\
              -a_{n1} & 0 & 0 & \dots & a_{11}
            \end{pmatrix}
            \begin{pmatrix}
              a_{11} & a_{12} & a_{13} & \dots & a_{1n} \\
              a_{21} & a_{22} & a_{23} & \dots & a_{2n} \\
              a_{31} & a_{32} & a_{33} & \dots & a_{3n} \\
              \vdots & \vdots & \vdots & \ddots & \vdots \\
              a_{n1} & a_{n2} & a_{n3} & \dots & a_{nn}
            \end{pmatrix}
            =
            \begin{pmatrix}
              a_{11} & a_{12} & a_{13} & \dots & a_{1n} \\
              0 & a'_{22} & a'_{23} & \dots & a'_{2n} \\
              0 & a'_{32} & a'_{33} & \dots & a'_{3n} \\
              \vdots & \vdots & \vdots & \ddots & \vdots \\
              0 & a'_{n2} & a'_{n3} & \dots & a'_{nn}
            \end{pmatrix}
          $$
          </div>
          <p>We denote the lower-triangular matrix $\mathbf{L}_1$.</p>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>The next step of Gaussian elimination can be written in a similar way:</p>
          <div style="font-size: 80%">
            $$
              \frac{1}{a'_{22}}
              \begin{pmatrix}
                a'_{22} & 0 & 0 & \dots & 0 \\
                0 & a'_{22} & 0 & \dots & 0 \\
                0 & -a'_{32} & a'_{22} & \dots & 0 \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                0 & -a'_{n2} & 0 & \dots & a'_{22}
              \end{pmatrix}
              \begin{pmatrix}
                a'_{11} & a'_{12} & a'_{13} & \dots & a'_{1n} \\
                0 & a'_{22} & a'_{23} & \dots & a'_{2n} \\
                0 & a'_{32} & a'_{33} & \dots & a'_{3n} \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                0 & a'_{n2} & a'_{n3} & \dots & a'_{nn}
              \end{pmatrix}
              =
              \begin{pmatrix}
                a'_{11} & a'_{12} & a'_{13} & \dots & a'_{1n} \\
                0 & a'_{22} & a'_{23} & \dots & a'_{2n} \\
                0 & 0 & a''_{33} & \dots & a''_{3n} \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & a''_{n3} & \dots & a''_{nn}
              \end{pmatrix}
            $$
          </div>
          <p>We denote the lower-triangular matrix $\mathbf{L}_2$.</p>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>After $n-1$ steps, we get:</p>
          $$
            \mathbf{L}_{n-1}\dots\mathbf{L}_2\mathbf{L}_1\mathbf{A} = \mathbf{U}
          $$
          <p>where $\mathbf{U}$ is upper-triangular.</p>
          <div class="fragment">
            <p>Up to now, this is simply a formalization of Gaussian elimination.
            If all the matrices $\mathbf{L}_i$ are invertible, we can write:</p>
            $$
              \mathbf{A} = \left(\mathbf{L}_1^{-1}\mathbf{L}_2^{-1}\dots\mathbf{L}_{n-1}^{-1}\right)\mathbf{U}
            $$
          </div>
        </section>

        <section>
          <h2>Gaussian Elimination</h2>
          <p>In fact, the inverse of the lower-triangular matrices are lower-triangular too:</p>
          <div style="font-size: 80%">
            $$
              \mathbf{L}_1^{-1} =
              \begin{pmatrix}
                1 & 0 & 0 & \dots & 0 \\
                -a_{21}/a_{11} & 1 & 0 & \dots & 0 \\
                -a_{31}/a_{11} & 0 & 1 & \dots & 0 \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                -a_{n1}/a_{11} & 0 & 0 & \dots & 1
              \end{pmatrix}^{-1}
              =
              \begin{pmatrix}
                1 & 0 & 0 & \dots & 0 \\
                a_{21}/a_{11} & 1 & 0 & \dots & 0 \\
                a_{31}/a_{11} & 0 & 1 & \dots & 0 \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                a_{n1}/a_{11} & 0 & 0 & \dots & 1
              \end{pmatrix}
            $$
          </div>
          <div class="fragment">
          <p>The product of the inverses of the lower triangular matrices
            $\mathbf{L}_i$ is again lower triangular, so we can write:</p>
          $$
            \mathbf{A} = \left(\mathbf{L}_1^{-1}\mathbf{L}_2^{-1}\dots\mathbf{L}_{n-1}^{-1}\right) = \mathbf{L}\mathbf{U}
          $$
          <p>This is called the <em>LU decomposition</em> (or factorization) of $\mathbf{A}$.</p>
          </div>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>LU decompositon refers to decomposing a square matrix into the
          product of a lower triangular matrix $\mathbf{L}$ and an upper triangular matrix $\mathbf{U}$.
          $$
            \mathbf{A} = \mathbf{L}\mathbf{U}
          $$
          </p>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>Typically, it is possible to take the diagonal elements of the
            lower-triangular matrix $\mathbf{L}$ to be all 1. In that case, we
            can write all the elements of both $\mathbf{L}$ and $\mathbf{U}$ neatly in
            a single matrix:</p>
          $$
          \begin{pmatrix}
          u_{11} & u_{12} & u_{13} & \dots & u_{1n} \\
          l_{21} & u_{22} & u_{23} & \dots & u_{2n} \\
          l_{31} & l_{32} & u_{33} & \dots & u_{3n} \\
          \vdots & \vdots & \vdots & \ddots & \vdots \\
          l_{n1} & l_{n2} & l_{n3} & \dots & u_{nn}
          \end{pmatrix}
          $$
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>A simple algorithm exists to find all the coefficients systematically. It's called <em>Crout's algorithm</em>.
          For each column with index $j$, we proceed from top to bottom:</p>
          <div style="font-size: 80%">
            $$
            \begin{align}
              u_{1j} &= a_{1j} \\
              u_{ij} &= a_{ij} - \sum_{k=1}^{i-1}l_{ik}u_{kj} & \text{for } i = 2, \dots, j \\
              l_{ij} &= \frac{1}{u_{jj}}\left(a_{ij} - \sum_{k=1}^{j-1}l_{ik}u_{kj}\right) & \text{for } i = j + 1, \dots, n
            \end{align}
            $$
          </div>
          <p class="fragment">Partial pivoting can be implemented by "promoting"
          one of the $l_{ij}$ to the diagonal.</p>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>It turns out we can always write a square matrix $\mathbf{A}$ as a
          product of a lower-triangular matrix $\mathbf{L}$ and an
          upper-triangular matrix $\mathbf{U}$ (potentially with some pivoting,
          denoted by the matrix $\mathbf{P}$):</p>
          $$
            \mathbf{P}\mathbf{A} = \mathbf{L}\mathbf{U}
          $$
          <p>This is called LU decomposition with partial pivoting, or LUP. For
          proof, see e.g. <a href="https://arxiv.org/abs/math/0506382">arXiv:math/0506382</a>.</p>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>Without pivoting, it is not guaranteed that the LU decomposition of
          matrix $\mathbf{A}$ exists. For example:</p>
          $$
          \begin{pmatrix}
          0 & 1 \\
          1 & 0
          \end{pmatrix} \neq \mathbf{L}\mathbf{U}
          $$
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>LU decomposition can be used for solving linear equations. The matrix equation:</p>
          $$
            \mathbf{A}\mathbf{x} = \mathbf{L}\mathbf{U}\mathbf{x} = \mathbf{b}
          $$
          <p>can be solved in two steps:</p>
          $$
            \mathbf{L}\mathbf{y} = \mathbf{b}
          $$
          $$
            \mathbf{U}\mathbf{x} = \mathbf{y}
          $$
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>We solve the first step using <em>forward substitution</em>:</p>
          $$
          \begin{align}
            y_1 &= \frac{b_1}{l_{11}} \\
            y_2 &= \frac{b_2 - l_{21}y_1}{l_{22}} \\
            \dots \\
            y_i &= \frac{1}{l_{ii}}\left(b_i - \sum_{j=1}^{i-1}l_{ij}y_j\right)
          \end{align}
          $$
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>Then we solve the second step using <em>backward substitution</em>:</p>
          $$
          \begin{align}
            x_n &= \frac{y_n}{u_{nn}} \\
            x_{n-1} &= \frac{y_{n-1} - u_{n-1,n}x_n}{u_{n-1,n-1}} \\
            \dots \\
            x_i &= \frac{1}{u_{ii}}\left(y_i - \sum_{j=i+1}^{n}u_{ij}x_j\right)
          \end{align}
          $$
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>Once we find the LU decomposition of matrix $\mathbf{A}$, we can
            use it to solve equations with any right hand side $\mathbf{b}$.</p>
          <div class="fragment">
            <p>For example, we can successively solve $n$ matrix equations with
            right hand side $\mathbf{b}_i$ from the identity matrix. This gives
            us the inverse of $\mathbf{A}$.
            </p>
          </div>
          <p class="fragment">However, it's rare that $\mathbf{A}^{-1}$ is
            used directly. For the purpose of repeatedly solving a linear system, it
          is often more advantageous to save the decomposition $\mathbf{L}\mathbf{U}$
          instead of the inverse itself.</p>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>It is also straightforward to find the determinant of $\mathbf{A}$, which is simply:</p>
          $$
          \det \mathbf{A} = (-1)^p\prod_{i=1}^{N} u_{ii}
          $$
          <p>where $p$ is the number of row exchanges we carried out with partial pivoting.</p>
        </section>


      </div>
    </div>
    <!-- <script src="js/head.min.js"></script> -->
    <!-- <script>
         head.js(
         "vendor/jquery.min.js",
         // "vendor/three.js/build/three.min.js",
         // "vendor/three.js/examples/js/controls/OrbitControls.js",
         /* "vendor/socket.io-client/dist/socket.io.js", */
         // "vendor/THREE.MeshLine.js",
         /* "vendor/dat.gui.min.js",*/
         "vendor/EventEmitter.js",
         // "js/samples.js",
         );
         </script> -->
    <script src="../dist/reveal.js"></script>
    <script src="../plugin/math/math.js"></script>
    <script src="../plugin/notes/notes.js"></script>
    <script src="../plugin/markdown/markdown.js"></script>
    <script src="../plugin/highlight/highlight.js"></script>

    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        transition: "fade",
        slideNumber: true,
        history: true,
        center: true,
        width: "100%",
        height: "100%",
        disableLayout: false,
        navigationMode: "default",
        // minScale: 1,
        // maxScale: 1,
        margin: 0.2,
        padding: 0,
        hash: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          // pass other options into `MathJax.Hub.Config()`
          TeX: { Macros: { RR: "{\\bf R}" } }
        },
        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
      });
      Reveal.configure({
        keyboard: {
          81: null, // Do not respond to Q
          // 32: null, // Do not respond to space
        }
      });
      // Reveal.addEventListener("slidechanged", function(event) {
      //   var foot1 = document.getElementById("footer1");
      //   var foot2 = document.getElementById("footer2");
      //
      //   if (Reveal.getIndices().h === 0) {
      //     foot1.style.display = "none";
      //     foot2.style.display = "none";
      //   } else {
      //     foot1.style.display = "block";
      //     foot2.style.display = "block";
      //   }
      // });
      //
    </script>
  </body>
</html>
