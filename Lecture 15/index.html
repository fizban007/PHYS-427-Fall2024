<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content=
      "width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Computational Physics Lecture 15</title>
    <link rel="stylesheet" href="../dist/reset.css">
    <link rel="stylesheet" href="../dist/reveal.css">
    <link rel="stylesheet" href="../dist/theme/serif.css"
      id="theme">
    <!-- <link rel="stylesheet" href="./dist/theme/night.css" id="theme"> -->
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href=
      "../plugin/highlight/monokai.css" id="highlight-theme">
    <link rel="stylesheet" href=
      "../css/style.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2>Numerical Linear Algebra</h2>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>Last time, we discussed the LU decomposition:
          $$
             \mathbf{A} = \mathbf{L}\mathbf{U}
          $$</p>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>The Python function <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html"><code>numpy.linalg.solve</code></a> wraps the LAPACK function <code>GESV</code>,
          which solves a linear system of equations
          $\mathbf{A}\mathbf{x}=\mathbf{b}$ using the LU decomposition.</p>
          <div class="fragment">
            <p>In general, <code>GESV</code> specifically uses LU decomposition
            with partial (row) pivoting. It is only numerically stable for <em>invertible</em> square
            matrices.</p>
            <p class="fragment">
              The scipy version <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve.html"><code>scipy.linalg.solve</code></a> can take a few extra parameters. One of them is <code>assume_a</code>,
              which can be used to specify if the matrix $\mathbf{A}$ is
              symmetric, hermitian, or positive definite. Then it will call
              specific routines in LAPACK to handle these specific cases.
            </p>
          </div>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>In the C++ library Eigen, there is a function called
            <a href="https://eigen.tuxfamily.org/dox/classEigen_1_1PartialPivLU.html"><code>PartialPivLU</code></a>,
            which computes the LU decomposition of a given matrix with partial
            pivoting. It is the default LU decomposition method in Eigen.</p>
          <p class="fragment">There is also a function called
            <a href="https://eigen.tuxfamily.org/dox/classEigen_1_1FullPivLU.html"><code>FullPivLU</code></a>,
            which computes the LU decomposition of a given matrix with full
            (including column) pivoting. It can work on matrices that are not
            invertible, and can reveal the rank of the matrix.</p>
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>The special case of tri-diagonal matrices is particularly simple to
          solve.</p>
          $$
          \begin{pmatrix}
          b_0 & c_0 & 0 & \dots & 0 & 0 \\
          a_1 & b_1 & c_1 & \dots & 0 & 0 \\
          0 & a_2 & b_2 & \dots & 0 & 0 \\
          \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
          0 & 0 & 0 & \dots & b_{n-2} & c_{n-2} \\
          0 & 0 & 0 & \dots & a_{n-1} & b_{n-1}
          \end{pmatrix}
          $$
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>Instead of storing the whole matrix with many zeros, we can
          store the vectors $\mathbf{a}$, $\mathbf{b}$, and $\mathbf{c}$.</p>
          <p class="fragment">It is customary to let all 3 vectors have $n$ elements,
            but set $a_0 = c_{n-1} = 0$.</p>
          <div class="fragment">
          <p>The LU decomposition of a tridiagonal matrix can be written as:</p>
          <div style="font-size: 80%;">
          $$
          \begin{pmatrix}
          b_0 & c_0 & 0 & \dots & 0 & 0 \\
          a_1 & b_1 & c_1 & \dots & 0 & 0 \\
          0 & a_2 & b_2 & \dots & 0 & 0 \\
          \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
          0 & 0 & 0 & \dots & b_{n-2} & c_{n-2} \\
          0 & 0 & 0 & \dots & a_{n-1} & b_{n-1}
         \end{pmatrix}
          =
          \begin{pmatrix}
         1 & 0 & 0 & \dots & 0 & 0 \\
          l_1 & 1 & 0 & \dots & 0 & 0 \\
          0 & l_2 & 1 & \dots & 0 & 0 \\
          \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
          0 & 0 & 0 & \dots & 1 & 0 \\
          0 & 0 & 0 & \dots & l_{n-1} & 1
          \end{pmatrix}
          \begin{pmatrix}
          u_0 & c_0 & 0 & \dots & 0 & 0 \\
          0 & u_1 & c_1 & \dots & 0 & 0 \\
          0 & 0 & u_2 & \dots & 0 & 0 \\
          \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
          0 & 0 & 0 & \dots & u_{n-2} & c_{n-2} \\
          0 & 0 & 0 & \dots & 0 & u_{n-1}
          \end{pmatrix}
          $$
          </div>
          </div>
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>It is easy to find the coefficients $l_i$ and $u_i$:</p>
          $$
          \begin{align}
          u_0 &= b_0 \\
          l_i &= a_i / u_{i-1} & \text{for } i = 1, \dots, n-1 \\
          u_i &= b_i - l_i c_{i-1} & \text{for } i = 1, \dots, n-1
          \end{align}
          $$
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>Then we still solve the linear system $\mathbf{A}\mathbf{x}=\mathbf{r}$ in two steps:</p>
          $$
          \mathbf{L}\mathbf{y} = \mathbf{r},\quad \mathbf{U}\mathbf{x} = \mathbf{y}
          $$
          <p>But each step can be written straightforwardly:</p>
          $$
          \begin{align}
          y_0 &= r_0 \\
          y_i &= r_i - l_i y_{i-1} & \text{for } i = 1, \dots, n-1 \\
          x_{n-1} &= y_{n-1} / u_{n-1} \\
          x_i &= \frac{y_i - c_i x_{i+1}}{u_i} & \text{for } i = n-2, \dots, 0
          \end{align}
          $$
          <p>Total of $\sim 3n$ operations only.</p>
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>Notice that there is no room for pivoting. Usually there is no
            need, since tridiagonal matrices arising from real applications
            typically satisfy the <em>diagonal dominance</em> condition:</p>
          $$
          |b_i| \geq |a_i| + |c_i| \quad \text{for } i = 0, \dots, n-1
          $$
          <p>which guarantees that the matrix is invertible.</p>
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>Do not use a general LU solver for tridiagonal matrices, since it
          will spend most of the time working on 0s.</p>
          <p class="fragment">You can typically implement a tridiagonal solver within 10 lines of
          code, so it's often practical to embed it into your program when you
          need it.</p>
          <p class="fragment">In general, one can use the LU form of the
          tridiagonal matrix $\mathbf{A}$ to compute the matrix product
          $\mathbf{A}^{-1}\mathbf{B}$, solving each column as a linear system.
          This is both faster and more accurate than finding the inverse.</p>
          <p class="fragment">If you really want to find the inverse
            $\mathbf{A}^{-1}$ explicitly, there is a general formula derived by <a href="https://www.sciencedirect.com/science/article/pii/0898122194900663?ref=cra_js_challenge&fr=RR-1">Usmami (1994)</a>. Note
            that the inverse is no longer sparse.</p>
        </section>

        <section>
          <h2>Cholesky Decomposition</h2>
          <p>When the matrix $\mathbf{A}$ is symmetric and positive-definite, we
          can define a special form of LU decomposition.</p>
          <ul>
            <li class="fragment">Symmetric means that $a_{ij} = a_{ji}$</li>
            <li class="fragment">Positive-definite means that for any vector $\mathbf{v}$, $\mathbf{v}\cdot\mathbf{A}\cdot\mathbf{v} > 0$. It
            is also equivalent to $\mathbf{A}$ having all positive eigenvalues</li>
          </ul>
        </section>

        <section>
          <h2>Cholesky Decomposition</h2>
          <p>When the matrix $\mathbf{A}$ is symmetric and positive-definite, we
          can define a special form of LU decomposition:</p>
          $$
          \mathbf{L}\mathbf{L}^T = \mathbf{A}
          $$
          <p>where $\mathbf{L}$ is a lower triangular matrix. This is a special
          case of LU decomposition where $\mathbf{U}$ is simply the transpose of
          $\mathbf{L}$.</p>
        </section>

        <section>
          <h2>Cholesky Decomposition</h2>
          <p>The components of $\mathbf{L}$ can be found using the following
          algorithm similar to Crout's algorithm:</p>
          $$
          \begin{align}
          L_{ii} &= \left(A_{ii} - \sum_{k=1}^{i-1}L_{ik}^2\right)^{1/2} \\
          L_{ji} &= \frac{1}{L_{ii}}\left(A_{ji} - \sum_{k=1}^{i-1}L_{jk}L_{ik}\right), \quad j > i
          \end{align}
          $$
          <p class="fragment">This is about a factor of 2 faster than normal LU decomposition. The
          algorithm is also always numerically stable, without the need of pivoting.</p>
          <p class="fragment">When the algorithm fails, it means the matrix $\mathbf{A}$ is not
          positive-definite. It is a relatively efficient way to test whether a symmetric matrix is positive-definite.</p>
        </section>

        <section>
          <h2>Cholesky Decomposition</h2>
          <p>Cholesky decomposition is useful for solving linear equations, just like LU
          decomposition. The only difference is that now $\mathbf{U}$ is the
          transpose of $\mathbf{L}$.</p>
          <p class="fragment">You can also think of Cholesky decomposition as
          finding the lower-triangular "square root" of the matrix $\mathbf{A}$.
          It is useful in simulating correlated random variables.</p>
        </section>

        <section>
          <h2>QR Decomposition</h2>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>Another useful matrix factorization method is the QR decomposition
          (NOT related to "QR codes"):</p>
          $$
          \mathbf{A} = \mathbf{Q}\mathbf{R}
          $$
          <p>where $\mathbf{Q}$ is an orthogonal matrix and $\mathbf{R}$ is an
          upper triangular matrix.</p>
          <p class="fragment">
            An orthogonal matrix is a matrix whose columns are
            orthonormal vectors. This means that $\mathbf{Q}^T\mathbf{Q} =
            \mathbf{I}$.
          </p>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>Suppose we know the QR decomposition of a matrix, we can use it to
          find the solution to the system of linear equations
          $\mathbf{A}\mathbf{x} = \mathbf{Q}\mathbf{R}\mathbf{x} = \mathbf{b}$.</p>
          <div class="fragment">
          <p>Since $\mathbf{Q}^T$ is the inverse of
          $\mathbf{Q}$, we can multiply both sides by $\mathbf{Q}^T$:</p>
          $$
          \mathbf{R}\mathbf{x} = \mathbf{Q}^T\mathbf{b}
          $$
          <p class="fragment">
            Because $\mathbf{R}$ is upper triangular, we can solve for
            $\mathbf{x}$ using back substitution.
          </p>
          </div>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>QR decomposition can also be used to determine the eigenvalues of a
          matrix $\mathbf{A}$.</p>
          <div class="fragment">
            <p>Suppose we already know the QR decomposition of the matrix,
            $\mathbf{A} = \mathbf{Q}\mathbf{R}$, we can form the following combination:</p>
            $$
            \mathbf{A}_1 = \mathbf{R}\mathbf{Q} = \mathbf{Q}^T\mathbf{A}\mathbf{Q}
            $$
          </div>
          <div class="fragment">
            <p>This is a similarity transformation that preserves the
            eigenvalues of the original matrix.</p>
          </div>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>Suppose we now find the QR decomposition of $\mathbf{A}_1$ and repeat this process:
          </p>
          $$
          \begin{align}
          \mathbf{A}_1 &= \mathbf{Q}_1 \mathbf{R}_1 \\
          \mathbf{A}_2 &= \mathbf{R}_1 \mathbf{Q}_1 = \mathbf{Q}_1^T\mathbf{A}_1\mathbf{Q}_1 \\
                       &= \mathbf{Q}_1^T\mathbf{Q}^T \mathbf{A} \mathbf{Q}\mathbf{Q}_1^T \\
          \dots
          \end{align}
          $$
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>It can be shown that this process eventually will converge. After
          many steps, the off-diagonal terms of $\mathbf{A}_n$ will become very
          small. Then $\mathbf{A}_n$ is now approximately diagonal, we can read
          off its eigenvalues from its diagonal components.</p>
          <div class="fragment">
            <p>Alternatively, the iteration may not yield a completely
            diagonal matrix, but will arrive at an upper-triangular matrix. This happens quite
              commonly, and the upper-triangular matrix
              is called the <em>Schur form</em> of $\mathbf{A}$, and the
              diagonal elements still correspond to the eigenvalues of $\mathbf{A}$.</p>
          </div>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>Let's see an example. Consider the following matrix</p>
          $$
          \mathbf{A} = \begin{pmatrix}
          4 & 2 & 1 \\
          2 & 1 & 0 \\
          1 & 0 & 3
          \end{pmatrix}
          $$
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>Its QR decomposition is (found by Mathematica):</p>
          $$
          \mathbf{A} = \mathbf{Q}_0\mathbf{R}_0 = \begin{pmatrix}
 0.87& 0.44& 0.22\\
 0.20& 0.10& -0.98\\
 0.45& -0.89& 0.
          \end{pmatrix}
          \begin{pmatrix}
 4.58& 2.18& 1.52\\
 0. & 0.49& -2.73\\
 0. & 0. & 0.45
          \end{pmatrix}
          $$
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>We proceed to form $\mathbf{A}_{k+1} = \mathbf{R}_k\mathbf{Q}_k$,
          and iterate. After 9 iterations, the resulting matrix is:</p>
          $$
          \mathbf{A}_9 = \begin{pmatrix}
 5.22& -0.13& 1.16\\
 0.003& 2.71& 0.21\\
 2.82\times 10^{-19} & -2.39\times 10^{-15} & -0.07\\
          \end{pmatrix}
          $$
          <div class="fragment">
            <p>The actual eigenvalues of the original matrix $\mathbf{A}$ are:</p>
            $$
            \lambda_1, \lambda_2, \lambda_3 = 5.35, 2.72, -0.07
            $$
          </div>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>In practice, iterating $\mathbf{A}_{k+1} =
          \mathbf{R}_k\mathbf{Q}_k$ can be incredibly slow to converge. The
          often-used strategy is to first reduce the matrix to tridiagonal form,
          then apply this QR algorithm.</p>
        </section>

        <section>
          <h2>Computing QR Decomposition</h2>
          <p>How do we compute the QR decomposition of a given matrix?</p>
          <div class="fragment">
            <p>Think of the original matrix as a set of $n$ individual columns:</p>
            $$
            \mathbf{A} = \begin{pmatrix}
            | & | & \dots & | \\
            \mathbf{a}_1 & \mathbf{a}_2 & \dots & \mathbf{a}_n \\
            | & | & \dots & |
            \end{pmatrix}
            $$
          </div>
        </section>

        <section>
          <h2>Computing QR Decomposition</h2>
          <p>The essence of the algorithm is to find the orthogonal matrix $\mathbf{Q}$ by constructing
          a set of orthonormal vectors $\{\mathbf{e}_i\}$.</p>
          <p class="fragment">Let's first define a set of orthogonal vectors:
          $$
          \begin{array}{ll}
          \mathbf{u}_1 = \mathbf{a}_1,& \mathbf{e}_1 = \mathbf{u}_1/|\mathbf{u}_1| \\
          \mathbf{u}_2 = \mathbf{a}_2 - (\mathbf{e}_1\cdot\mathbf{a}_2)\mathbf{e}_1,& \mathbf{e}_2 = \mathbf{u}_2/|\mathbf{u}_2| \\
          \mathbf{u}_3 = \mathbf{a}_3 - (\mathbf{e}_1\cdot\mathbf{a}_3)\mathbf{e}_1 - (\mathbf{e}_2\cdot\mathbf{a}_3)\mathbf{e}_2,& \mathbf{e}_3 = \mathbf{u}_3/|\mathbf{u}_3| \\
          \ldots & \\
          \mathbf{u}_i = \mathbf{a}_i - \sum_{j=1}^{i-1}(\mathbf{e}_j\cdot\mathbf{a}_i)\mathbf{e}_j,& \mathbf{e}_i = \mathbf{u}_i/|\mathbf{u}_i| \\
          \end{array}
          $$
          </p>
        </section>

        <section>
          <h2>Computing QR Decomposition</h2>
          <p>Rearranging the definition of these vectors, we can write the
          original vectors of $\mathbf{A}$ as:</p>
          $$
          \begin{align}
          \mathbf{a}_1 &= |\mathbf{u}_1|\mathbf{e}_1 \\
          \mathbf{a}_2 &= |\mathbf{u}_2|\mathbf{e}_2 + (\mathbf{e}_1\cdot\mathbf{a}_2)\mathbf{e}_1 \\
          \mathbf{a}_3 &= |\mathbf{u}_3|\mathbf{e}_3 + (\mathbf{e}_1\cdot\mathbf{a}_3)\mathbf{e}_1 + (\mathbf{e}_2\cdot\mathbf{a}_3)\mathbf{e}_2 \\
          \ldots
          \end{align}
          $$
          <p class="fragment">
            This set of equations can be written in matrix form.
          </p>
        </section>

        <section>
          <h2>Computing QR Decomposition</h2>
          $$
          \begin{pmatrix}
          | & | & \dots & | \\
          \mathbf{a}_1 & \mathbf{a}_2 & \dots & \mathbf{a}_n \\
          | & | & \dots & | \\
          | & | & \dots & |
          \end{pmatrix}
          =
          \begin{pmatrix}
          | & | & \dots & | \\
          \mathbf{e}_1 & \mathbf{e}_2 & \dots & \mathbf{e}_n \\
          | & | & \dots & | \\
          | & | & \dots & |
          \end{pmatrix}
          \begin{pmatrix}
          |\mathbf{u}_1| & (\mathbf{e}_1\cdot\mathbf{a}_2) & \dots & (\mathbf{e}_1\cdot\mathbf{a}_n) \\
          0 & |\mathbf{u}_2| & \dots & (\mathbf{e}_2\cdot\mathbf{a}_n) \\
          \vdots & \vdots & \ddots & \vdots \\
          0 & 0 & \dots & |\mathbf{u}_n|
          \end{pmatrix}
          $$
          <p class="fragment">The matrices on the right hand side are precisely our
          $\mathbf{Q}$ and $\mathbf{R}$. $\mathbf{Q}$ is an orthogonal matrix
          because the set of vectors $\mathbf{e}_i$ is orthonormal,
          $\mathbf{e}_i\cdot\mathbf{e}_j = \delta_{ij}$. Therefore $\mathbf{Q}\mathbf{Q}^T = \mathbf{1}$.</p>
        </section>

        <section>
          <h2>Gram-Schmidt Process</h2>
          <p>The algorithm I described is called the Gram-Schmidt process. Unfortunately
          this process is numerically unstable, especially for large
          matrices. The often implemented QR decomposition algorithm is instead
          based on <em>Householder transformations</em>.</p>
          <p class="fragment">The Gram-Schmidt process uses repeated projections
            to get rid of undesired components of the matrix. Householder transformations use <em>reflections</em> instead.</p>
        </section>

        <section>
          <h2>Householder Transformations</h2>
          <p>Given an arbitrary matrix $\mathbf{A}$, the goal is to use orthogonal transformations
          to make it an upper triangular matrix. We can start with a single vector $\mathbf{a}$
          and attempt to construct a transformation that sets all its components to zero except the
          first one:
          $$
            \mathbf{a} = \begin{pmatrix}
            a_1 \\
            a_2 \\
            \vdots \\
            a_n
            \end{pmatrix},\quad
            \mathbf{P}\mathbf{a} = \begin{pmatrix}
            a \\
            0 \\
            \vdots \\
            0
            \end{pmatrix}
          $$</p>
          <p class="fragment">The matrix $\mathbf{P}$ should be an orthogonal matrix.</p>
        </section>

        <section>
          <h2>Householder Transformations</h2>
          <p>This is achieved using the Householder matrix:
          $$
          \mathbf{P} = \mathbf{I} - 2\mathbf{v}\mathbf{v}^T,\quad \mathbf{v} = \frac{\mathbf{u}}{|\mathbf{u}|},
          \quad \mathbf{u} = \mathbf{a} - |\mathbf{a}|\mathbf{e}_1
          $$
          </p>
          <div class="fragment">
            <p>The matrix $\mathbf{P}$ is orthogonal, and is its own inverse.
              $\mathbf{P} = \mathbf{P}^{-1} = \mathbf{P}^T$.</p>
          </div>
          <p class="fragment">
            The geometric interpretation of the Householder matrix is that it
            reflects the vector $\mathbf{a}$ across a plane perpendicular to
            $\mathbf{v}$.
          </p>
        </section>

        <section>
          <h2>Householder Transformations</h2>
          <p>To set all the lower elements of the first column $\mathbf{a}_1$ to
          zero, we form our first
          Householder matrix $\mathbf{P}_1$, then multiply it to $\mathbf{A}$.</p>
          <div class="fragment">
            <p>Then, to set the lower elements of the second column
            $\mathbf{a}_2$ to zero, we take the $\mathbf{x}$ to be the lower
            $n-1$ elements of $\mathbf{a}_2$ and form a $(n-1)\times(n-1)$
            Householder matrix $\mathbf{P}_2$.</p>
          </div>
          <p class="fragment">
            We can continue this process to get an upper triangular matrix. The
            orthogonal matrix $\mathbf{Q}$ is the product of all the Householder
            matrices, since $\mathbf{P}_i$ is its own inverse:
            $$
            \mathbf{P}_n\mathbf{P}_{n-1}\dots\mathbf{P}_2\mathbf{P}_1\mathbf{A} = \mathbf{R},\quad \mathbf{A} = (\mathbf{P}_1\mathbf{P}_2\dots\mathbf{P}_n)\mathbf{R} = \mathbf{Q}\mathbf{R}
            $$
          </p>
        </section>

        <section>
          <h2>Householder Transformations</h2>
          <p>Householder transformation is the most widely implemented algorithm
          for QR decomposition.</p>
          <p class="fragment">For example, the <code>Eigen</code> C++ library contains the <a href="https://eigen.tuxfamily.org/dox/classEigen_1_1HouseholderQR.html"><code>HouseholderQR</code></a> module
            that calculates the QR decomposition of a matrix using Householder
            transformations. See <a href="https://eigen.tuxfamily.org/dox/group__TutorialLinearAlgebra.html">this page</a> for
            a comparison of various decompositions for solving linear equations.</p>
          <p class="fragment">
            The Python library <a href="https://numpy.org/devdocs/reference/generated/numpy.linalg.qr.html"><code>numpy.linalg.qr</code></a> wraps
            the LAPACK function <code>dgeqrf</code>. It is also based on the
            Householder algorithm.
          </p>
        </section>

        <section>
          <h2>Householder Transformations</h2>
          <p>One can also use a series of Householder transformations to bring a
          symmetric matrix to tri-diagonal form, then use the
          $\mathbf{R}\mathbf{Q}$ iteration method to find its eigenvalues.</p>
          <p class="fragment">
            This is a much more efficient way to compute the eigenvalues of a
            matrix than brute-force iteration with QR decomposition.
          </p>
          <p class="fragment">
            In practice, you can always use a reputable library when you want to
            compute the eigenvalues and/or eigenvectors of a matrix. For
            example, Python has <code>numpy.linalg.eig</code> for
            a general $n\times n$ matrix, and <code>numpy.linalg.eigh</code> for
            a real symmetric matrix or complex Hermitian matrix.
          </p>
        </section>

      </div>
    </div>
    <!-- <script src="js/head.min.js"></script> -->
    <!-- <script>
         head.js(
         "vendor/jquery.min.js",
         // "vendor/three.js/build/three.min.js",
         // "vendor/three.js/examples/js/controls/OrbitControls.js",
         /* "vendor/socket.io-client/dist/socket.io.js", */
         // "vendor/THREE.MeshLine.js",
         /* "vendor/dat.gui.min.js",*/
         "vendor/EventEmitter.js",
         // "js/samples.js",
         );
         </script> -->
    <script src="../dist/reveal.js"></script>
    <script src="../plugin/math/math.js"></script>
    <script src="../plugin/notes/notes.js"></script>
    <script src="../plugin/markdown/markdown.js"></script>
    <script src="../plugin/highlight/highlight.js"></script>

    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        transition: "fade",
        slideNumber: true,
        history: true,
        center: true,
        width: "100%",
        height: "100%",
        disableLayout: false,
        navigationMode: "default",
        // minScale: 1,
        // maxScale: 1,
        margin: 0.2,
        padding: 0,
        hash: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          // pass other options into `MathJax.Hub.Config()`
          TeX: { Macros: { RR: "{\\bf R}" } }
        },
        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
      });
      Reveal.configure({
        keyboard: {
          81: null, // Do not respond to Q
          // 32: null, // Do not respond to space
        }
      });
      // Reveal.addEventListener("slidechanged", function(event) {
      //   var foot1 = document.getElementById("footer1");
      //   var foot2 = document.getElementById("footer2");
      //
      //   if (Reveal.getIndices().h === 0) {
      //     foot1.style.display = "none";
      //     foot2.style.display = "none";
      //   } else {
      //     foot1.style.display = "block";
      //     foot2.style.display = "block";
      //   }
      // });
      //
    </script>
  </body>
</html>
