<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content=
      "width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Computational Physics Lecture 15</title>
    <link rel="stylesheet" href="../dist/reset.css">
    <link rel="stylesheet" href="../dist/reveal.css">
    <link rel="stylesheet" href="../dist/theme/serif.css"
      id="theme">
    <!-- <link rel="stylesheet" href="./dist/theme/night.css" id="theme"> -->
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href=
      "../plugin/highlight/monokai.css" id="highlight-theme">
    <link rel="stylesheet" href=
      "../css/style.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2>Numerical Linear Algebra</h2>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>Last time, we discussed the LU decomposition:
          $$
             \mathbf{A} = \mathbf{L}\mathbf{U}
          $$</p>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>The Python function <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html"><code>numpy.linalg.solve</code></a> wraps the LAPACK function <code>GESV</code>,
          which solves a linear system of equations
          $\mathbf{A}\mathbf{x}=\mathbf{b}$ using the LU decomposition.</p>
          <div class="fragment">
            <p>In general, <code>GESV</code> specifically uses LU decomposition
            with partial (row) pivoting. It is only numerically stable for <em>invertible</em> square
            matrices.</p>
            <p class="fragment">
              The scipy version <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve.html"><code>scipy.linalg.solve</code></a> can take a few extra parameters. One of them is <code>assume_a</code>,
              which can be used to specify if the matrix $\mathbf{A}$ is
              symmetric, hermitian, or positive definite. Then it will call
              specific routines in LAPACK to handle these specific cases.
            </p>
          </div>
        </section>

        <section>
          <h2>LU Decomposition</h2>
          <p>In the C++ library Eigen, there is a function called
            <a href="https://eigen.tuxfamily.org/dox/classEigen_1_1PartialPivLU.html"><code>PartialPivLU</code></a>,
            which computes the LU decomposition of a given matrix with partial
            pivoting. It is the default LU decomposition method in Eigen.</p>
          <p class="fragment">There is also a function called
            <a href="https://eigen.tuxfamily.org/dox/classEigen_1_1FullPivLU.html"><code>FullPivLU</code></a>,
            which computes the LU decomposition of a given matrix with full
            (including column) pivoting. It can work on matrices that are not
            invertible, and can reveal the rank of the matrix.</p>
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>The special case of tri-diagonal matrices is particularly simple to
          solve.</p>
          $$
          \begin{pmatrix}
          b_0 & c_0 & 0 & \dots & 0 & 0 \\
          a_1 & b_1 & c_1 & \dots & 0 & 0 \\
          0 & a_2 & b_2 & \dots & 0 & 0 \\
          \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
          0 & 0 & 0 & \dots & b_{n-2} & c_{n-2} \\
          0 & 0 & 0 & \dots & a_{n-1} & b_{n-1}
          \end{pmatrix}
          $$
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>Instead of storing the whole matrix with many zeros, we can
          store the vectors $\mathbf{a}$, $\mathbf{b}$, and $\mathbf{c}$.</p>
          <p class="fragment">It is customary to let all 3 vectors have $n$ elements,
            but set $a_0 = c_{n-1} = 0$.</p>
          <div class="fragment">
          <p>The LU decomposition of a tridiagonal matrix can be written as:</p>
          <div style="font-size: 80%;">
          $$
          \begin{pmatrix}
          b_0 & c_0 & 0 & \dots & 0 & 0 \\
          a_1 & b_1 & c_1 & \dots & 0 & 0 \\
          0 & a_2 & b_2 & \dots & 0 & 0 \\
          \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
          0 & 0 & 0 & \dots & b_{n-2} & c_{n-2} \\
          0 & 0 & 0 & \dots & a_{n-1} & b_{n-1}
         \end{pmatrix}
          =
          \begin{pmatrix}
         1 & 0 & 0 & \dots & 0 & 0 \\
          l_1 & 1 & 0 & \dots & 0 & 0 \\
          0 & l_2 & 1 & \dots & 0 & 0 \\
          \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
          0 & 0 & 0 & \dots & 1 & 0 \\
          0 & 0 & 0 & \dots & l_{n-1} & 1
          \end{pmatrix}
          \begin{pmatrix}
          u_0 & c_0 & 0 & \dots & 0 & 0 \\
          0 & u_1 & c_1 & \dots & 0 & 0 \\
          0 & 0 & u_2 & \dots & 0 & 0 \\
          \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
          0 & 0 & 0 & \dots & u_{n-2} & c_{n-2} \\
          0 & 0 & 0 & \dots & 0 & u_{n-1}
          \end{pmatrix}
          $$
          </div>
          </div>
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>It is easy to find the coefficients $l_i$ and $u_i$:</p>
          $$
          \begin{align}
          u_0 &= b_0 \\
          l_i &= a_i / u_{i-1} & \text{for } i = 1, \dots, n-1 \\
          u_i &= b_i - l_i c_{i-1} & \text{for } i = 1, \dots, n-1
          \end{align}
          $$
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>Then we still solve the linear system $\mathbf{A}\mathbf{x}=\mathbf{r}$ in two steps:</p>
          $$
          \mathbf{L}\mathbf{y} = \mathbf{r},\quad \mathbf{U}\mathbf{x} = \mathbf{y}
          $$
          <p>But each step can be written straightforwardly:</p>
          $$
          \begin{align}
          y_0 &= r_0 \\
          y_i &= r_i - l_i y_{i-1} & \text{for } i = 1, \dots, n-1 \\
          x_{n-1} &= y_{n-1} / u_{n-1} \\
          x_i &= \frac{y_i - c_i x_{i+1}}{u_i} & \text{for } i = n-2, \dots, 0
          \end{align}
          $$
          <p>Total of $\sim 3n$ operations only.</p>
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>Notice that there is no room for pivoting. Usually there is no
            need, since tridiagonal matrices arising from real applications
            typically satisfy the <em>diagonal dominance</em> condition:</p>
          $$
          |b_i| \geq |a_i| + |c_i| \quad \text{for } i = 0, \dots, n-1
          $$
          <p>which guarantees that the matrix is invertible.</p>
        </section>

        <section>
          <h2>Tri-diagonal Matrices</h2>
          <p>Do not use a general LU solver for tridiagonal matrices, since it
          will spend most of the time working on 0s.</p>
          <p class="fragment">You can typically implement a tridiagonal solver within 10 lines of
          code, so it's often practical to embed it into your program when you
          need it.</p>
          <p class="fragment">In general, one can use the LU form of the
          tridiagonal matrix $\mathbf{A}$ to compute the matrix product
          $\mathbf{A}^{-1}\mathbf{B}$, solving each column as a linear system.
          This is both faster and more accurate than finding the inverse.</p>
          <p class="fragment">If you really want to find the inverse
            $\mathbf{A}^{-1}$ explicitly, there is a general formula derived by <a href="https://www.sciencedirect.com/science/article/pii/0898122194900663?ref=cra_js_challenge&fr=RR-1">Usmami (1994)</a>. Note
            that the inverse is no longer sparse.</p>
        </section>

        <section>
          <h2>Cholesky Decomposition</h2>
          <p>When the matrix $\mathbf{A}$ is symmetric and positive-definite, we
          can define a special form of LU decomposition.</p>
          <ul>
            <li class="fragment">Symmetric means that $a_{ij} = a_{ji}$</li>
            <li class="fragment">Positive-definite means that for any vector $\mathbf{v}$, $\mathbf{v}\cdot\mathbf{A}\cdot\mathbf{v} > 0$. It
            is also equivalent to $\mathbf{A}$ having all positive eigenvalues</li>
          </ul>
        </section>

        <section>
          <h2>Cholesky Decomposition</h2>
          <p>When the matrix $\mathbf{A}$ is symmetric and positive-definite, we
          can define a special form of LU decomposition:</p>
          $$
          \mathbf{L}\mathbf{L}^T = \mathbf{A}
          $$
          <p>where $\mathbf{L}$ is a lower triangular matrix. This is a special
          case of LU decomposition where $\mathbf{U}$ is simply the transpose of
          $\mathbf{L}$.</p>
        </section>

        <section>
          <h2>Cholesky Decomposition</h2>
          <p>The components of $\mathbf{L}$ can be found using the following
          algorithm similar to Crout's algorithm:</p>
          $$
          \begin{align}
          L_{ii} &= \left(A_{ii} - \sum_{k=1}^{i-1}L_{ik}^2\right)^{1/2} \\
          L_{ji} &= \frac{1}{L_{ii}}\left(A_{ji} - \sum_{k=1}^{i-1}L_{jk}L_{ik}\right), \quad j > i
          \end{align}
          $$
          <p class="fragment">This is about a factor of 2 faster than normal LU decomposition. The
          algorithm is also always numerically stable, without the need of pivoting.</p>
          <p class="fragment">When the algorithm fails, it means the matrix $\mathbf{A}$ is not
          positive-definite. It is a relatively efficient way to test whether a symmetric matrix is positive-definite.</p>
        </section>

        <section>
          <h2>Cholesky Decomposition</h2>
          <p>Cholesky decomposition is useful for solving linear equations, just like LU
          decomposition. The only difference is that now $\mathbf{U}$ is the
          transpose of $\mathbf{L}$.</p>
          <p class="fragment">You can also think of Cholesky decomposition as
          finding the lower-triangular "square root" of the matrix $\mathbf{A}$.
          It is useful in simulating correlated random variables.</p>
        </section>

        <section>
          <h2>QR Decomposition</h2>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>Another useful matrix factorization method is the QR decomposition
          (NOT related to "QR codes"):</p>
          $$
          \mathbf{A} = \mathbf{Q}\mathbf{R}
          $$
          <p>where $\mathbf{Q}$ is an orthogonal matrix and $\mathbf{R}$ is an
          upper triangular matrix.</p>
          <p class="fragment">
            An orthogonal matrix is a matrix whose columns are
            orthonormal vectors. This means that $\mathbf{Q}^T\mathbf{Q} =
            \mathbf{I}$.
          </p>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>Suppose we know the QR decomposition of a matrix, we can use it to
          find the solution to the system of linear equations
          $\mathbf{A}\mathbf{x} = \mathbf{Q}\mathbf{R}\mathbf{x} = \mathbf{b}$.</p>
          <div class="fragment">
          <p>Since $\mathbf{Q}^T$ is the inverse of
          $\mathbf{Q}$, we can multiply both sides by $\mathbf{Q}^T$:</p>
          $$
          \mathbf{R}\mathbf{x} = \mathbf{Q}^T\mathbf{b}
          $$
          <p class="fragment">
            Because $\mathbf{R}$ is upper triangular, we can solve for
            $\mathbf{x}$ using back substitution.
          </p>
          </div>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>QR decomposition can also be used to determine the eigenvalues of a
          matrix $\mathbf{A}$.</p>
          <div class="fragment">
            <p>Suppose we already know the QR decomposition of the matrix,
            $\mathbf{A} = \mathbf{Q}\mathbf{R}$, we can form the following combination:</p>
            $$
            \mathbf{A}_1 = \mathbf{R}\mathbf{Q} = \mathbf{Q}^T\mathbf{A}\mathbf{Q}
            $$
          </div>
          <div class="fragment">
            <p>This is a similarity transformation that preserves the
            eigenvalues of the original matrix.</p>
          </div>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>Suppose we now find the QR decomposition of $\mathbf{A}_1$ and repeat this process:
          </p>
          $$
          \begin{align}
          \mathbf{A}_1 &= \mathbf{Q}_1 \mathbf{R}_1 \\
          \mathbf{A}_2 &= \mathbf{R}_1 \mathbf{Q}_1 = \mathbf{Q}_1^T\mathbf{A}_1\mathbf{Q}_1 \\
                       &= \mathbf{Q}_1^T\mathbf{Q}^T \mathbf{A} \mathbf{Q}\mathbf{Q}_1^T \\
          \dots
          \end{align}
          $$
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>It can be shown that this process eventually will converge. After
          many steps, the off-diagonal terms of $\mathbf{A}_n$ will become very
          small. Then $\mathbf{A}_n$ is now approximately diagonal, we can read
          off its eigenvalues from its diagonal components.</p>
          <div class="fragment">
            <p>Alternatively, the iteration may not yield a completely
            diagonal matrix, but will arrive at an upper-triangular matrix. This happens quite
              commonly, and the upper-triangular matrix
              is called the <em>Schur form</em> of $\mathbf{A}$, and the
              diagonal elements still correspond to the eigenvalues of $\mathbf{A}$.</p>
          </div>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>Let's see an example. Consider the following matrix</p>
          $$
          \mathbf{A} = \begin{pmatrix}
          4 & 2 & 1 \\
          2 & 1 & 0 \\
          1 & 0 & 3
          \end{pmatrix}
          $$
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>Its QR decomposition is (found by Mathematica):</p>
          $$
          \mathbf{A} = \mathbf{Q}_0\mathbf{R}_0 = \begin{pmatrix}
 0.87& 0.44& 0.22\\
 0.20& 0.10& -0.98\\
 0.45& -0.89& 0.
          \end{pmatrix}
          \begin{pmatrix}
 4.58& 2.18& 1.52\\
 0. & 0.49& -2.73\\
 0. & 0. & 0.45
          \end{pmatrix}
          $$
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>We proceed to form $\mathbf{A}_{k+1} = \mathbf{R}_k\mathbf{Q}_k$,
          and iterate. After 9 iterations, the resulting matrix is:</p>
          $$
          \mathbf{A}_9 = \begin{pmatrix}
 5.22& -0.13& 1.16\\
 0.003& 2.71& 0.21\\
 2.82\times 10^{-19} & -2.39\times 10^{-15} & -0.07\\
          \end{pmatrix}
          $$
          <div class="fragment">
            <p>The actual eigenvalues of the original matrix $\mathbf{A}$ are:</p>
            $$
            \lambda_1, \lambda_2, \lambda_3 = 5.35, 2.72, -0.07
            $$
          </div>
        </section>

        <section>
          <h2>QR Decomposition</h2>
          <p>In practice, iterating $\mathbf{A}_{k+1} =
          \mathbf{R}_k\mathbf{Q}_k$ can be incredibly slow to converge. The
          often-used strategy is to first reduce the matrix to tridiagonal form,
          then apply this QR algorithm.</p>
        </section>


      </div>
    </div>
    <!-- <script src="js/head.min.js"></script> -->
    <!-- <script>
         head.js(
         "vendor/jquery.min.js",
         // "vendor/three.js/build/three.min.js",
         // "vendor/three.js/examples/js/controls/OrbitControls.js",
         /* "vendor/socket.io-client/dist/socket.io.js", */
         // "vendor/THREE.MeshLine.js",
         /* "vendor/dat.gui.min.js",*/
         "vendor/EventEmitter.js",
         // "js/samples.js",
         );
         </script> -->
    <script src="../dist/reveal.js"></script>
    <script src="../plugin/math/math.js"></script>
    <script src="../plugin/notes/notes.js"></script>
    <script src="../plugin/markdown/markdown.js"></script>
    <script src="../plugin/highlight/highlight.js"></script>

    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        transition: "fade",
        slideNumber: true,
        history: true,
        center: true,
        width: "100%",
        height: "100%",
        disableLayout: false,
        navigationMode: "default",
        // minScale: 1,
        // maxScale: 1,
        margin: 0.2,
        padding: 0,
        hash: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          // pass other options into `MathJax.Hub.Config()`
          TeX: { Macros: { RR: "{\\bf R}" } }
        },
        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
      });
      Reveal.configure({
        keyboard: {
          81: null, // Do not respond to Q
          // 32: null, // Do not respond to space
        }
      });
      // Reveal.addEventListener("slidechanged", function(event) {
      //   var foot1 = document.getElementById("footer1");
      //   var foot2 = document.getElementById("footer2");
      //
      //   if (Reveal.getIndices().h === 0) {
      //     foot1.style.display = "none";
      //     foot2.style.display = "none";
      //   } else {
      //     foot1.style.display = "block";
      //     foot2.style.display = "block";
      //   }
      // });
      //
    </script>
  </body>
</html>
